{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should change your bucket name below. See the book for more details\n",
    "# (https://www.amazon.com/Data-Analytics-Google-Cloud-Hands-ebook/dp/B087XZZ2C6/)\n",
    "data = spark.read.csv(\"gs://rs-movielens-2/ratings.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(userId=1, movieId=1, rating=4.0, timestamp=964982703), Row(userId=1, movieId=3, rating=4.0, timestamp=964981247)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 1, 4.0, 964982703], [1, 3, 4.0, 964981247]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.rdd.take(2)) # viewing 2 rows of the raw rdd\n",
    "\n",
    "# Convert your dataframe into an RDD, and then into a list\n",
    "ratings = data.rdd.map(list)\n",
    "ratings.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1, product=1, rating=4.0), Rating(user=1, product=3, rating=4.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Rating object\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "# Convert the data into Rating objects\n",
    "ratings_data = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])));\n",
    "# This is what a Rating object looks like\n",
    "ratings_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test, in 80-20% ratio\n",
    "training_data, test_data = ratings_data.randomSplit([0.8,0.2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ALS method\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "# Build the model based on the training data, with tank = 10 and iterations = 10\n",
    "model = ALS.train(training_data, rank=10, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1), (1, 6)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the ratings column\n",
    "testdata_nr = test_data.map(lambda p: (p[0],p[1]))\n",
    "testdata_nr.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the model  \n",
    "predictions = model.predictAll(testdata_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=590, product=1084, rating=3.8205984311223125),\n",
       " Rating(user=414, product=1084, rating=4.12221289922118)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first rows of the RDD\n",
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), 4.0), ((1, 3), 4.0)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare ratings data\n",
    "ratings_kv = ratings_data.map(lambda r: ((r[0],r[1]),r[2]));\n",
    "ratings_kv.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((590, 1084), 3.8205984311223125), ((414, 1084), 4.12221289922118)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare predictions data\n",
    "predictions_kv = predictions.map(lambda r: ((r[0],r[1]),r[2]))\n",
    "predictions_kv.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1219), (2.0, 4.913380814462766)),\n",
       " ((1, 1377), (3.0, 3.155047446018721)),\n",
       " ((1, 1552), (4.0, 3.9478755850642604)),\n",
       " ((1, 1644), (3.0, 2.8124819971676938)),\n",
       " ((1, 1804), (5.0, 1.782714730433351))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the ratings data with predictions data\n",
    "ratings_predictions = ratings_kv.join(predictions_kv)\n",
    "ratings_predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      " |-- _3: double (nullable = true)\n",
      " |-- _4: double (nullable = true)\n",
      "\n",
      "+---+----+---+------------------+\n",
      "| _1|  _2| _3|                _4|\n",
      "+---+----+---+------------------+\n",
      "|  1|1219|2.0| 4.913380814462766|\n",
      "|  1|1377|3.0| 3.155047446018721|\n",
      "|  1|1552|4.0|3.9478755850642604|\n",
      "|  1|1644|3.0|2.8124819971676938|\n",
      "|  1|1804|5.0| 1.782714730433351|\n",
      "+---+----+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert RDD to Dataframe\n",
    "# First, create a clean version of the ratings_predictions RDD\n",
    "ratings_predictions_clean = ratings_predictions. \\\n",
    "  map(lambda r: (r[0][0], r[0][1], r[1][0], r[1][1]))\n",
    "# Next, convert this clean version to a PySpark dataframe\n",
    "df_ratings_predictions = ratings_predictions_clean.toDF()\n",
    "# Print the dataframe schema - just for your information\n",
    "df_ratings_predictions.printSchema()\n",
    "# Print some records in the dataframe - just for your information\n",
    "df_ratings_predictions.show(5)\n",
    "# Save the dataframe df_ratings_predictions into a \n",
    "# single csv file using \"coalesce\" command\n",
    "df_ratings_predictions.coalesce(1).write.format('com.databricks.spark.csv'). \\\n",
    "save('gs://rs-movielens-2/ratings_predictions1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of the model for the test data = 0.8240\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print MAE\n",
    "MAE = ratings_predictions.map(lambda r: abs(r[1][0] - r[1][1])).mean()\n",
    "print(\"Mean Absolute Error of the model for the test data = {:.4f}\".format(MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of the model for the test data = 1.2286\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print MSE\n",
    "MSE = ratings_predictions.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error of the model for the test data = {:.4f}\".format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error of the model for the test data = 1.1084\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print RMSE\n",
    "RMSE = MSE**(1/2)\n",
    "print(\"Root Mean Squared Error of the model for the test data = \\\n",
    "{:.4f}\".format(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}