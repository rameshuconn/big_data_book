{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should change your bucket name below. See the book for more details\n",
    "(https://www.amazon.com/Data-Analytics-Google-Cloud-Hands-ebook/dp/B087XZZ2C6/)\n",
    "data = spark.read.csv(\"gs://rs-movielens-2/ratings.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(userId=1, movieId=1, rating=4.0, timestamp=964982703), Row(userId=1, movieId=3, rating=4.0, timestamp=964981247)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 1, 4.0, 964982703], [1, 3, 4.0, 964981247]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.rdd.take(2)) # viewing 2 rows of the raw rdd\n",
    "\n",
    "# Convert your dataframe into an RDD, and then into a list\n",
    "ratings = data.rdd.map(list)\n",
    "ratings.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=1, product=1, rating=4.0), Rating(user=1, product=3, rating=4.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the Rating object\n",
    "from pyspark.mllib.recommendation import Rating\n",
    "# Convert the data into Rating objects\n",
    "ratings_data = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])));\n",
    "# This is what a Rating object looks like\n",
    "ratings_data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test, in 80-20% ratio\n",
    "training_data, test_data = ratings_data.randomSplit([0.8,0.2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ALS method\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "# Build the model based on the training data, with tank = 10 and iterations = 10\n",
    "model = ALS.train(training_data, rank=10, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3), (1, 151)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the ratings column\n",
    "testdata_nr = test_data.map(lambda p: (p[0],p[1]))\n",
    "testdata_nr.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the model  \n",
    "predictions = model.predictAll(testdata_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=414, product=1084, rating=4.2084196107141425),\n",
       " Rating(user=294, product=1084, rating=2.79318157043616)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the first rows of the RDD\n",
    "predictions.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), 4.0), ((1, 3), 4.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare ratings data\n",
    "ratings_kv = ratings_data.map(lambda r: ((r[0],r[1]),r[2]));\n",
    "ratings_kv.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((414, 1084), 4.2084196107141425), ((294, 1084), 2.79318157043616)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare predictions data\n",
    "predictions_kv = predictions.map(lambda r: ((r[0],r[1]),r[2]))\n",
    "predictions_kv.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 3), (4.0, 3.211911768374442)),\n",
       " ((1, 151), (5.0, 3.2276032841269178)),\n",
       " ((1, 457), (5.0, 4.7321496727891255)),\n",
       " ((1, 919), (5.0, 3.9240397529605335)),\n",
       " ((1, 1089), (5.0, 4.7363555272243305))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the ratings data with predictions data\n",
    "ratings_predictions = ratings_kv.join(predictions_kv)\n",
    "ratings_predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      " |-- _3: double (nullable = true)\n",
      " |-- _4: double (nullable = true)\n",
      "\n",
      "+---+----+---+------------------+\n",
      "| _1|  _2| _3|                _4|\n",
      "+---+----+---+------------------+\n",
      "|  1|   3|4.0| 3.211911768374442|\n",
      "|  1| 151|5.0|3.2276032841269178|\n",
      "|  1| 457|5.0|4.7321496727891255|\n",
      "|  1| 919|5.0|3.9240397529605335|\n",
      "|  1|1089|5.0|4.7363555272243305|\n",
      "+---+----+---+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert RDD to Dataframe\n",
    "# First, create a clean version of the ratings_predictions RDD\n",
    "ratings_predictions_clean = ratings_predictions.map(lambda r: (r[0][0], r[0][1], r[1][0], r[1][1]))\n",
    "# Next, convert this clean version to a PySpark dataframe\n",
    "df_ratings_predictions = ratings_predictions_clean.toDF()\n",
    "# Print the dataframe schema - just for your information\n",
    "df_ratings_predictions.printSchema()\n",
    "# Print some records in the dataframe - just for your information\n",
    "df_ratings_predictions.show(5)\n",
    "# Save the dataframe df_ratings_predictions into a single csv file using \"coalesce\" command\n",
    "df_ratings_predictions.coalesce(1).write.format('com.databricks.spark.csv').save('gs://rs-movielens-2/ratings_predictions1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error of the model for the test data = 0.8411\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print MAE\n",
    "MAE = ratings_predictions.map(lambda r: abs(r[1][0] - r[1][1])).mean()\n",
    "print(\"Mean Absolute Error of the model for the test data = {:.4f}\".format(MAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of the model for the test data = 1.2746\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print MSE\n",
    "MSE = ratings_predictions.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error of the model for the test data = {:.4f}\".format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error of the model for the test data = 1.1290\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print RMSE\n",
    "#import org.apache.spark.sql.functions.sqrt\n",
    "import math\n",
    "RMSE = MSE**(1/2)\n",
    "print(\"Root Mean Squared Error of the model for the test data = {:.4f}\".format(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}