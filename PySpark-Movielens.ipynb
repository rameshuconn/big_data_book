{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["data = spark.read.csv(\"gs://rs-movielens-1/ratings.csv\", header=True, inferSchema=True)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+-------+------+---------+\n","|userId|movieId|rating|timestamp|\n","+------+-------+------+---------+\n","|     1|      1|   4.0|964982703|\n","|     1|      3|   4.0|964981247|\n","|     1|      6|   4.0|964982224|\n","|     1|     47|   5.0|964983815|\n","|     1|     50|   5.0|964982931|\n","+------+-------+------+---------+\n","only showing top 5 rows\n","\n"]}],"source":["data.show(5)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[Row(userId=1, movieId=1, rating=4.0, timestamp=964982703), Row(userId=1, movieId=3, rating=4.0, timestamp=964981247)]\n"]},{"data":{"text/plain":["[[1, 1, 4.0, 964982703], [1, 3, 4.0, 964981247]]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["print(data.rdd.take(2)) # viewing 2 rows of the raw rdd\n","\n","# Convert your dataframe into an RDD, and then into a list\n","ratings = data.rdd.map(list)\n","ratings.take(2)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["[Rating(user=1, product=1, rating=4.0), Rating(user=1, product=3, rating=4.0)]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Import the Rating object\n","from pyspark.mllib.recommendation import Rating\n","# Convert the data into Rating objects\n","ratings_data = ratings.map(lambda line: Rating(int(line[0]), int(line[1]), float(line[2])));\n","# This is what a Rating object looks like\n","ratings_data.take(2)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Split the data into training and test, in 80-20% ratio\n","training_data, test_data = ratings_data.randomSplit([0.8,0.2]);"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Import the ALS method\n","from pyspark.mllib.recommendation import ALS\n","# Build the model based on the training data, with tank = 10 and iterations = 10\n","model = ALS.train(training_data, rank=10, iterations=10)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["[(1, 3), (1, 163)]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Drop the ratings column\n","testdata_nr = test_data.map(lambda p: (p[0],p[1]))\n","testdata_nr.take(2)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# Predict the model  \n","predictions = model.predictAll(testdata_nr)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["[Rating(user=32, product=1084, rating=4.121777284904251),\n"," Rating(user=4, product=1084, rating=4.00846783159416)]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Print the first rows of the RDD\n","predictions.take(2)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["[((1, 1), 4.0), ((1, 3), 4.0)]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Prepare ratings data\n","ratings_kv = ratings_data.map(lambda r: ((r[0],r[1]),r[2]));\n","ratings_kv.take(2)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["[((32, 1084), 4.121777284904251), ((4, 1084), 4.00846783159416)]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Prepare predictions data\n","predictions_kv = predictions.map(lambda r: ((r[0],r[1]),r[2]))\n","predictions_kv.take(2)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["[((1, 3), (4.0, 4.162075958965319)), ((1, 316), (3.0, 4.537450779249192))]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Join the ratings data with predictions data\n","ratings_predictions = ratings_kv.join(predictions_kv)\n","ratings_predictions.take(2)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean Squared Error of the model for the test data = 1.27\n"]}],"source":["# Calculate and print MSE\n","MSE = ratings_predictions.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n","print(\"Mean Squared Error of the model for the test data = {:.2f}\".format(MSE))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":2}